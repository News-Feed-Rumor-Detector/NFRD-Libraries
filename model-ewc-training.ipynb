{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm \nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer, RobertaForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EWC:\n    \n    def __init__(self, prior_model, data_samples, num_sample=30, lambda_=0.1):\n        self.prior_model = prior_model\n        self.prior_weights = [weight.cpu().detach().numpy() for weight in prior_model.parameters()]\n        self.num_sample = num_sample\n        self.data_samples = data_samples\n        self.fisher_matrix = self.compute_fisher()\n        self.lambda_ = lambda_  # Define lambda_ attribute\n        \n    def compute_fisher(self):\n        weights = self.prior_weights\n        fisher_accum = [np.zeros_like(layer) for layer in weights]\n        criterion = torch.nn.CrossEntropyLoss()\n        for j in tqdm(range(self.num_sample)):\n            idx = np.random.randint(self.data_samples.shape[0])\n            input_sample = self.data_samples[idx].unsqueeze(0).to(self.prior_model.device)  # Ensure sample is on the same device as the model\n            outputs = self.prior_model(input_sample)\n            loss = criterion(outputs.logits, torch.tensor([0]).to(outputs.logits.device))  # Assume binary classification\n            gradients = torch.autograd.grad(outputs=loss, inputs=self.prior_model.parameters(), create_graph=True)\n            for m, grad in enumerate(gradients):\n                fisher_accum[m] += grad.detach().cpu().numpy() ** 2  # Move gradient to CPU, detach, and convert to NumPy array\n        fisher_accum = [fisher / self.num_sample for fisher in fisher_accum]\n        return fisher_accum\n    \n    def compute_penalty_loss(self, model):\n        penalty = 0.\n        for fisher, param, param_prior in zip(self.fisher_matrix, model.parameters(), self.prior_model.parameters()):\n            param_numpy = param.detach().cpu().numpy()  # Convert tensor to NumPy array\n            param_prior_numpy = param_prior.detach().cpu().numpy()  # Convert tensor to NumPy array\n            penalty += torch.sum(torch.tensor(fisher) * ((param_numpy - param_prior_numpy) ** 2))  # Convert NumPy array to tensor\n        return 0.5 * self.lambda_ * penalty\n    \n    def get_fisher(self):\n        return self.fisher_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_ewc(model, tokenizer, ewc_texts):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    ewc_encodings = tokenizer(ewc_texts, truncation=True, padding=True, return_tensors='pt')\n    ewc_encodings = {key: value.to(device) for key, value in ewc_encodings.items()}\n    data_samples = ewc_encodings['input_ids']  # Sample a small subset of data\n    ewc = EWC(prior_model=model, data_samples=data_samples, num_sample=400, lambda_=10)\n    return ewc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, ewc=None, epochs=20, learning_rate=1e-5, patience=5, model_save_path='bazina/nfrd-model'):\n    # Set up optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    # Initialize early stopping variables\n    best_val_loss = float('inf')    \n    no_improvement_counter = 0\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        train_correct_predictions = 0\n        train_total_samples = 0\n\n        for batch in train_loader:\n            input_ids, attention_mask, labels = batch\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            \n            # Calculate additional EWC loss\n            if ewc is not None:\n                ewc_loss = ewc.compute_penalty_loss(model)\n                loss += ewc_loss\n            \n            loss.backward()\n            optimizer.step()\n\n            # Calculate training accuracy\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            train_correct_predictions += (predictions == labels).sum().item()\n            train_total_samples += labels.size(0)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids, attention_mask, labels = batch\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=1)\n                correct_predictions += (predictions == labels).sum().item()\n                total_samples += labels.size(0)\n\n        average_val_loss = val_loss / len(val_loader)\n        accuracy = correct_predictions / total_samples\n        train_accuracy = train_correct_predictions / train_total_samples\n\n        print(f'Epoch {epoch + 1}/{epochs}, '\n              f'Train Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, '\n              f'Val Loss: {average_val_loss:.4f}, Val Accuracy: {accuracy:.4f}')\n\n        # Check for early stopping\n        if average_val_loss < best_val_loss:\n            best_val_loss = average_val_loss\n            no_improvement_counter = 0\n\n            # Save the model\n            model.push_to_hub(model_save_path)\n            tokenizer.push_to_hub(model_save_path)\n            print(f'Model uploaded to the Hugging Face Model Hub')\n        else:\n            no_improvement_counter += 1\n\n        if no_improvement_counter >= patience:\n            print(f'Early stopping after {epoch + 1} epochs without improvement.')\n            break\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model_with_ewc(train_texts, val_texts, train_labels, val_labels, ewc_texts, model_save_path='bazina/nfrd-model'):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    tokenizer = AutoTokenizer.from_pretrained('bazina/nfrd-model')\n    model = RobertaForSequenceClassification.from_pretrained('bazina/nfrd-model', num_labels=2)\n    \n    # Initialize EWC\n    ewc = initialize_ewc(model, tokenizer, ewc_texts)\n    \n    # Tokenize and encode the training and validation sets\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')\n    \n    # Convert data to PyTorch tensors and move them to GPU\n    train_encodings = {key: value.to(device) for key, value in train_encodings.items()}\n    val_encodings = {key: value.to(device) for key, value in val_encodings.items()}\n    train_labels = torch.tensor(train_labels).to(device)\n    val_labels = torch.tensor(val_labels).to(device)\n    \n    # Create PyTorch datasets\n    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n    val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n    \n    # Create PyTorch data loaders\n    batch_size = 4\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Fine-tune model with EWC\n    new_model = fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, model_save_path='bazina/nfrd-model')\n    \n    return new_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# usage\n# train_model_with_ewc(train_texts, val_texts, train_labels, val_labels, ewc_texts, model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:08:38.367038Z","iopub.execute_input":"2024-05-10T14:08:38.367444Z","iopub.status.idle":"2024-05-10T14:08:38.386834Z","shell.execute_reply.started":"2024-05-10T14:08:38.367414Z","shell.execute_reply":"2024-05-10T14:08:38.386066Z"},"trusted":true},"execution_count":1,"outputs":[]}]}