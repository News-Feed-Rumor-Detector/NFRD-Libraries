{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:39:12.523649Z","iopub.status.busy":"2024-05-22T08:39:12.523056Z","iopub.status.idle":"2024-05-22T08:39:36.776616Z","shell.execute_reply":"2024-05-22T08:39:36.775676Z","shell.execute_reply.started":"2024-05-22T08:39:12.523617Z"},"trusted":true},"outputs":[],"source":["!pip install supabase"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:39:36.778587Z","iopub.status.busy":"2024-05-22T08:39:36.778286Z","iopub.status.idle":"2024-05-22T08:39:49.835283Z","shell.execute_reply":"2024-05-22T08:39:49.834105Z","shell.execute_reply.started":"2024-05-22T08:39:36.778560Z"},"trusted":true},"outputs":[],"source":["!pip install psycopg2-binary\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T11:55:04.913416Z","iopub.status.busy":"2024-05-22T11:55:04.912483Z","iopub.status.idle":"2024-05-22T11:55:05.871714Z","shell.execute_reply":"2024-05-22T11:55:05.870573Z","shell.execute_reply.started":"2024-05-22T11:55:04.913378Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory '/root/.kaggle': File exists\n"]}],"source":["!mkdir ~/.kaggle\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T11:55:09.643477Z","iopub.status.busy":"2024-05-22T11:55:09.643103Z","iopub.status.idle":"2024-05-22T11:55:10.596309Z","shell.execute_reply":"2024-05-22T11:55:10.595086Z","shell.execute_reply.started":"2024-05-22T11:55:09.643445Z"},"trusted":true},"outputs":[],"source":["!cp /kaggle/input/kaggle-token/kaggle.json ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:40:04.369667Z","iopub.status.busy":"2024-05-22T08:40:04.369312Z","iopub.status.idle":"2024-05-22T08:40:12.098917Z","shell.execute_reply":"2024-05-22T08:40:12.097901Z","shell.execute_reply.started":"2024-05-22T08:40:04.369640Z"},"trusted":true},"outputs":[],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import numpy as np\n","from kaggle_secrets import UserSecretsClient\n","from supabase import create_client, ClientOptions\n","import torch\n","from tqdm import tqdm\n","from sklearn.neighbors import NearestNeighbors\n","from statistics import mode\n","import psycopg2\n","import os\n","import pandas as pd\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:40:12.100844Z","iopub.status.busy":"2024-05-22T08:40:12.100421Z","iopub.status.idle":"2024-05-22T08:40:12.111534Z","shell.execute_reply":"2024-05-22T08:40:12.110567Z","shell.execute_reply.started":"2024-05-22T08:40:12.100820Z"},"trusted":true},"outputs":[],"source":["# Function to extract RoBERTa features for a batch of encodings\n","def extract_roberta_features_batch(model, encodings):\n","    input_ids = torch.tensor(encodings['input_ids'])\n","    attention_mask = torch.tensor(encodings['attention_mask'])\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        last_hidden_states = outputs.last_hidden_state[:, 0, :].detach().numpy()\n","\n","    return last_hidden_states\n","\n","# Function to extract RoBERTa features for the entire data\n","def extract_roberta_features(model, encodings, batch_size=32):\n","    num_samples = len(encodings.input_ids)\n","    num_batches = (num_samples + batch_size - 1) // batch_size\n","\n","    features = []\n","    for i in tqdm(range(num_batches), desc=\"Extracting RoBERTa Features\"):\n","        start_idx = i * batch_size\n","        end_idx = min((i + 1) * batch_size, num_samples)\n","        batch_encodings = {key: value[start_idx:end_idx] for key, value in encodings.items()}\n","        batch_features = extract_roberta_features_batch(model, batch_encodings)\n","        features.append(batch_features)\n","\n","    features = np.concatenate(features, axis=0)\n","    return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:40:15.878245Z","iopub.status.busy":"2024-05-22T08:40:15.877448Z","iopub.status.idle":"2024-05-22T08:40:15.885455Z","shell.execute_reply":"2024-05-22T08:40:15.884187Z","shell.execute_reply.started":"2024-05-22T08:40:15.878183Z"},"trusted":true},"outputs":[],"source":["def fetch_data_from_supabase(schema, table, field, num_of_records=None):\n","    user_secrets = UserSecretsClient()\n","\n","    # Connect to supabase\n","    supabase_url = \"https://fglqovplibiyttjzqxuj.supabase.co\"\n","    supabase_key = user_secrets.get_secret(\"SUPABASE_KEY\")\n","\n","    supabase = create_client(\n","                supabase_url,\n","                supabase_key,\n","                options=ClientOptions(\n","                  schema=schema\n","                ))\n","\n","\n","    # Execute query\n","    if num_of_records is not None:\n","        resp = supabase.table(table).select(field).limit(num_of_records).execute()\n","    else:\n","        resp = supabase.table(table).select(field).execute()\n","\n","    data = resp.data\n","\n","    data = [row[field] for row in data]\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:42:57.310596Z","iopub.status.busy":"2024-05-22T08:42:57.309578Z","iopub.status.idle":"2024-05-22T08:42:57.318220Z","shell.execute_reply":"2024-05-22T08:42:57.317276Z","shell.execute_reply.started":"2024-05-22T08:42:57.310563Z"},"trusted":true},"outputs":[],"source":["class WeightedKNNClassifier:\n","    def __init__(self, n_neighbors=5):\n","        self.n_neighbors = n_neighbors\n","\n","    def fit(self, X_train, y_train, weights):\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.weights = weights\n","\n","    def predict(self, X_test):\n","        y_pred = []\n","        for x in X_test:\n","            distances = np.sqrt(np.sum((self.X_train - x) ** 2 * self.weights, axis=1))\n","            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n","            nearest_labels = self.y_train[nearest_indices].astype(int)  # Cast nearest_indices to int\n","            pred_label = np.bincount(nearest_labels).argmax()\n","            y_pred.append(pred_label)\n","        return np.array(y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:04:37.569999Z","iopub.status.busy":"2024-05-22T09:04:37.569613Z","iopub.status.idle":"2024-05-22T09:04:37.586521Z","shell.execute_reply":"2024-05-22T09:04:37.585378Z","shell.execute_reply.started":"2024-05-22T09:04:37.569970Z"},"trusted":true},"outputs":[],"source":["def propagate_labels(labeled_texts, labeled_texts_labels, unlabeled_texts, weights=None):\n","    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","    model = RobertaModel.from_pretrained('roberta-base')\n","\n","    # Load the CSV files of the features lakes\n","    df_true = pd.read_csv('/kaggle/input/features-lake/true_features_lake.csv')\n","    df_false = pd.read_csv('/kaggle/input/features-lake/false_features_lake.csv')\n","\n","    # Extract the \"features\" column and store them in lists\n","    true_features = df_true['feature'].apply(eval).tolist()  # Convert string representation back to list\n","    false_features = df_false['feature'].apply(eval).tolist()  # Convert string representation back to list\n","\n","\n","    # Add labels and prepare data\n","    all_features = true_features + false_features\n","    all_labels = [0] * len(true_features) + [1] * len(false_features)\n","\n","    # Tokenize and encode the text data\n","    labeled_texts_encodings = tokenizer(labeled_texts, truncation=True, padding=True)\n","    unlabeled_texts_encodings = tokenizer(unlabeled_texts, truncation=True, padding=True)\n","\n","    # Extract the features of the encodings using RoBERTa\n","    labeled_texts_features = extract_roberta_features(model, labeled_texts_encodings)\n","    unlabeled_texts_features = extract_roberta_features(model, unlabeled_texts_encodings)\n","\n","    # Concatenate the labeled texts features of this batch with the whole set of labeled features\n","    concatenated_features = np.concatenate((all_features, labeled_texts_features), axis=0)\n","    concatenated_labels = all_labels + labeled_texts_labels\n","\n","    # Reshape the features to fill two dimensions\n","    concatenated_features_2d = concatenated_features.reshape(concatenated_features.shape[0], -1)\n","    unlabeled_texts_features_2d = unlabeled_texts_features.reshape(unlabeled_texts_features.shape[0], -1)\n","\n","    pad_width = concatenated_features_2d.shape[1] - unlabeled_texts_features_2d.shape[1]\n","    unlabeled_texts_features_2d = np.pad(unlabeled_texts_features_2d, ((0, 0), (0, pad_width)), mode='constant')\n","\n","    # Write the new features to the set of saved features\n","    # Prepare new records\n","    true_records = []\n","    false_records = []\n","\n","    labeled_texts_features_2d = labeled_texts_features.reshape(labeled_texts_features.shape[0], -1)\n","    for index in range(len(labeled_texts_features_2d)):\n","        feature_str = str(labeled_texts_features_2d[index].tolist())  # Convert list to string for storage\n","        if labeled_texts_labels[index] == 0:\n","            true_records.append({\"id\": len(df_true) + index, \"feature\": feature_str})\n","        elif labeled_texts_labels[index] == 1:\n","            false_records.append({\"id\": len(df_false) + index, \"feature\": feature_str})\n","\n","    # Convert lists of dictionaries to DataFrames\n","    df_true_new = pd.DataFrame(true_records)\n","    df_false_new = pd.DataFrame(false_records)\n","\n","    # Concatenate the new records with the existing DataFrames\n","    df_true = pd.concat([df_true, df_true_new], ignore_index=True)\n","    df_false = pd.concat([df_false, df_false_new], ignore_index=True)\n","\n","    os.makedirs('/kaggle/working/updated_dataset', exist_ok=True)\n","    # Save the updated dataframes back to CSV files\n","    df_true.to_csv('/kaggle/working/updated_dataset/true_features_lake.csv', index=False)\n","    df_false.to_csv('/kaggle/working/updated_dataset/false_features_lake.csv', index=False)\n","\n","    metadata = {\n","    \"title\": \"features-lake\",\n","    \"id\": \"nfrdkaggle/features-lake\",\n","    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n","    }\n","\n","    with open('/kaggle/working/updated_dataset/dataset-metadata.json', 'w') as f:\n","        json.dump(metadata, f)\n","\n","\n","    # Train KNN classifier\n","    if weights is None:\n","        knn_classifier = KNeighborsClassifier(n_neighbors=5)\n","        knn_classifier.fit(concatenated_features_2d, concatenated_labels)\n","        predictions = knn_classifier.predict(unlabeled_texts_features_2d)\n","    else:\n","        knn_classifier = WeightedKNNClassifier(n_neighbors=5)\n","        knn_classifier.fit(concatenated_features_2d, concatenated_labels, weights=weights)\n","        predictions = knn_classifier.predict(unlabeled_texts_features_2d)\n","\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T08:43:06.944384Z","iopub.status.busy":"2024-05-22T08:43:06.943508Z","iopub.status.idle":"2024-05-22T08:43:06.955277Z","shell.execute_reply":"2024-05-22T08:43:06.954329Z","shell.execute_reply.started":"2024-05-22T08:43:06.944350Z"},"trusted":true},"outputs":[],"source":["def perform_representative_replacement(features, labels):\n","    # Calculate distances between data points\n","    nn = NearestNeighbors(n_neighbors=len(features), metric='euclidean')\n","    nn.fit(features)\n","    distances, indices = nn.kneighbors(features)\n","\n","    # Set a threshold for similarity\n","    threshold = 0.7\n","\n","   # Check if labels are already numeric\n","    if all(isinstance(label, int) for label in labels):\n","        numerical_labels = labels\n","    else:\n","        label_map = {'true': 0, 'false': 1}\n","        numerical_labels = [label_map[label.lower()] for label in labels]\n","\n","    # Group similar data points\n","    similar_groups = {}\n","    for i in range(len(features)):\n","        similar_group = [(i, numerical_labels[i])]\n","        for j, dist in zip(indices[i], distances[i]):\n","            if j != i and dist < threshold:\n","                similar_group.append((j, numerical_labels[j]))\n","        if len(similar_group) > 1:\n","            similar_groups[i] = similar_group\n","\n","    # Select representative data points and calculate weights\n","    representatives = []\n","    representatives_labels = []\n","    weights = []\n","    for group in similar_groups.values():\n","        group_X = [features[i] for i, j in group]\n","        group_Y = [j for i, j in group]  # Index train_labels with integers\n","        # Calculate weight based on the number of data points in the group\n","        weight = len(group)\n","        weights.append([weight for i in range(len(features[0]))])\n","        # Select a representative data point\n","        representative = np.mean(group_X, axis=0)\n","        representative_label = mode(group_Y)\n","        representatives.append(representative)\n","        representatives_labels.append(representative_label)\n","\n","\n","    # Replace similar data points with representatives\n","    summarized_features = np.array(representatives)\n","    summarized_labels = np.array(representatives_labels)\n","    summarized_weights = np.array(weights)\n","\n","    return summarized_features, summarized_labels, summarized_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T12:09:50.813668Z","iopub.status.busy":"2024-05-22T12:09:50.813311Z","iopub.status.idle":"2024-05-22T12:09:52.049989Z","shell.execute_reply":"2024-05-22T12:09:52.048997Z","shell.execute_reply.started":"2024-05-22T12:09:50.813638Z"},"trusted":true},"outputs":[],"source":["!kaggle datasets version -p /kaggle/working/updated_dataset -m \"Updated dataset with new features\""]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5060233,"sourceId":8483473,"sourceType":"datasetVersion"},{"datasetId":5062281,"sourceId":8486150,"sourceType":"datasetVersion"},{"modelInstanceId":28441,"sourceId":33979,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
