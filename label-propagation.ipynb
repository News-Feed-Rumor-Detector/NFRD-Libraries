{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":33979,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28441}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install supabase","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install psycopg2-binary\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom kaggle_secrets import UserSecretsClient\nfrom supabase import create_client, ClientOptions\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.neighbors import NearestNeighbors\nfrom statistics import mode\nimport psycopg2\nimport os\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract RoBERTa features for a batch of encodings\ndef extract_roberta_features_batch(model, encodings):\n    input_ids = torch.tensor(encodings['input_ids']) \n    attention_mask = torch.tensor(encodings['attention_mask'])\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        last_hidden_states = outputs.last_hidden_state[:, 0, :].detach().numpy()\n    \n    return last_hidden_states\n\n# Function to extract RoBERTa features for the entire data\ndef extract_roberta_features(model, encodings, batch_size=32):\n    num_samples = len(encodings.input_ids)\n    num_batches = (num_samples + batch_size - 1) // batch_size\n    \n    features = []\n    for i in tqdm(range(num_batches), desc=\"Extracting RoBERTa Features\"):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, num_samples)\n        batch_encodings = {key: value[start_idx:end_idx] for key, value in encodings.items()}\n        batch_features = extract_roberta_features_batch(model, batch_encodings)\n        features.append(batch_features)\n    \n    features = np.concatenate(features, axis=0)\n    return features\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_data_from_supabase(schema, table, field, num_of_records=None):\n    user_secrets = UserSecretsClient()\n\n    # Connect to supabase\n    supabase_url = \"https://fglqovplibiyttjzqxuj.supabase.co\"\n    supabase_key = user_secrets.get_secret(\"SUPABASE_KEY\")\n\n    supabase = create_client(\n                supabase_url,\n                supabase_key,\n                options=ClientOptions(\n                  schema=schema\n                ))\n\n\n    # Execute query\n    if num_of_records is not None:\n        resp = supabase.table(table).select(field).limit(num_of_records).execute()\n    else:\n        resp = supabase.table(table).select(field).execute()\n    \n    data = resp.data\n\n    data = [row[field] for row in data]\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_the_whole_features(labels_type):\n    # Retrieve database password from Kaggle Secrets\n    user_secrets = UserSecretsClient()\n    database_password = user_secrets.get_secret(\"DATABASE_PASSWORD\")\n\n    # Database connection URL\n    connection_url = f\"postgres://postgres.fglqovplibiyttjzqxuj:{database_password}@aws-0-eu-central-1.pooler.supabase.com:5432/postgres\"\n    \n    query = f\"SELECT feature FROM features_lake.{labels_type}_features_lake\"\n    features = [] #init\n\n    try:\n        # Establish a connection to the database\n        connection = psycopg2.connect(connection_url)\n\n        # Create a cursor object to execute SQL queries\n        cursor = connection.cursor()\n        cursor.execute(\"SET statement_timeout TO 3600000;\")\n        cursor.close()\n        \n        # Execute the SQL query\n        cursor = connection.cursor()\n        cursor.execute(query)\n\n        # Fetch all the result rows\n        new_records = cursor.fetchall()\n\n        # Process the new records\n        for record in new_records:\n            features.append(record[0])\n            \n        # Commit the transaction\n        connection.commit()\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while connecting to PostgreSQL:\", error)\n\n    finally:\n        # Close the cursor and connection\n        if connection:\n            cursor.close()\n            connection.close()\n            print(\"PostgreSQL connection is closed\")\n            \n    return features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_records_to_supabase(schema, table, records):\n    user_secrets = UserSecretsClient()\n\n    # Connect to Supabase\n    supabase_url = \"https://fglqovplibiyttjzqxuj.supabase.co\"\n    supabase_key = user_secrets.get_secret(\"SUPABASE_KEY\")\n\n    supabase = create_client(\n                supabase_url,\n                supabase_key,\n                options=ClientOptions(\n                  schema=schema\n                ))\n\n    try:\n        # Insert records into Supabase\n        for record in records:\n            supabase.table(table).insert(record).execute()\n    except APIError as e:\n        print(\"API Error occurred:\", e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_features(features, labels, title, dim):\n    pca = PCA(n_components=dim)\n    reduced_features = pca.fit_transform(features)\n\n    # Convert labels to numerical values\n    label_map = {'true': 0, 'false': 1}\n    numerical_labels = [label_map[label] for label in labels]\n    \n    if (dim == 3):\n        # Plot the reduced features in 3D\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(reduced_features[:, 0], reduced_features[:, 1], reduced_features[:, 2], c=numerical_labels, cmap='viridis', alpha=0.5)\n        ax.set_title(title)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_zlabel('Principal Component 3')\n        plt.show()\n    elif (dim == 2):\n        # Plot the reduced features in 2d\n        plt.figure(figsize=(10, 6))\n        plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=numerical_labels, cmap='viridis', alpha=0.5)\n        plt.title(title)\n        plt.xlabel('Principal Component 1')\n        plt.ylabel('Principal Component 2')\n        plt.colorbar(label='Label')\n        plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightedKNNClassifier:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X_train, y_train, weights):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.weights = weights\n\n    def predict(self, X_test):\n        y_pred = []\n        for x in X_test:\n            distances = np.sqrt(np.sum((self.X_train - x) ** 2 * self.weights, axis=1))\n            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n            nearest_labels = self.y_train[nearest_indices].astype(int)  # Cast nearest_indices to int\n            pred_label = np.bincount(nearest_labels).argmax()\n            y_pred.append(pred_label)\n        return np.array(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def propagate_labels(labeled_texts, labeled_texts_labels, unlabeled_texts, weights=None):\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    model = RobertaModel.from_pretrained('roberta-base')\n    \n    true_features = fetch_the_whole_features(\"true\")\n    false_features = fetch_the_whole_features(\"false\")\n\n    # Add labels and prepare data\n    all_features = true_features + false_features\n    all_labels = [\"true\"] * len(true_features) + [\"false\"] * len(false_features)\n\n    # Tokenize and encode the text data\n    labeled_texts_encodings = tokenizer(labeled_texts, truncation=True, padding=True)\n    unlabeled_texts_encodings = tokenizer(unlabeled_texts, truncation=True, padding=True)\n\n    # Extract the features of the encodings using RoBERTa\n    labeled_texts_features = extract_roberta_features(model, labeled_texts_encodings)\n    unlabeled_texts_features = extract_roberta_features(model, unlabeled_texts_encodings)\n    \n    # Concatenate the labeled texts features of this batch with the whole set of labeled features\n    concatenated_features = np.concatenate((all_features, labeled_texts_features), axis=0)\n    concatenated_labels = all_labels + labeled_texts_labels\n    \n    # Reshape the features to fill two dimensions\n    concatenated_features_2d = concatenated_features.reshape(concatenated_features.shape[0], -1)\n    unlabeled_texts_features_2d = unlabeled_texts_features.reshape(unlabeled_texts_features.shape[0], -1)\n\n    pad_width = concatenated_features_2d.shape[1] - unlabeled_texts_features_2d.shape[1]\n    unlabeled_texts_features_2d = np.pad(unlabeled_texts_features_2d, ((0, 0), (0, pad_width)), mode='constant')\n    \n    # Write the new features to the set of saved features\n    true_records = []\n    false_records = []\n    for index in range(len(labeled_texts_features)):\n        if labeled_texts_labels[index] == \"true\":\n            true_records.append({\"feature\": labeled_texts_features[index]})\n        elif labeled_texts_labels[index] == \"false\":\n            false_records.append({\"feature\": labeled_texts_features[index]})\n\n    write_records_to_supabase(\"features_lake\", \"true_features_lake\", true_records)\n    write_records_to_supabase(\"features_lake\", \"false_features_lake\", false_records)\n\n    # Train KNN classifier\n    if weights is None:\n        knn_classifier = KNeighborsClassifier(n_neighbors=5)\n        knn_classifier.fit(concatenated_features_2d, concatenated_labels)\n        predictions = knn_classifier.predict(unlabeled_texts_features_2d)\n    else:\n        knn_classifier = WeightedKNNClassifier(n_neighbors=5)\n        knn_classifier.fit(concatenated_features_2d, concatenated_labels, weights=weights)\n        predictions = knn_classifier.predict(unlabeled_texts_features_2d)\n\n\n    return predictions\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_representative_replacement(features, labels):\n    # Calculate distances between data points\n    nn = NearestNeighbors(n_neighbors=len(features), metric='euclidean')\n    nn.fit(features)\n    distances, indices = nn.kneighbors(features)\n\n    # Set a threshold for similarity\n    threshold = 0.7\n\n   # Check if labels are already numeric\n    if all(isinstance(label, int) for label in labels):\n        numerical_labels = labels\n    else:\n        label_map = {'true': 0, 'false': 1}\n        numerical_labels = [label_map[label.lower()] for label in labels]\n\n    # Group similar data points\n    similar_groups = {}\n    for i in range(len(features)):\n        similar_group = [(i, numerical_labels[i])]\n        for j, dist in zip(indices[i], distances[i]):\n            if j != i and dist < threshold:\n                similar_group.append((j, numerical_labels[j]))\n        if len(similar_group) > 1:  \n            similar_groups[i] = similar_group\n\n    # Select representative data points and calculate weights\n    representatives = []\n    representatives_labels = []\n    weights = []\n    for group in similar_groups.values():\n        group_X = [features[i] for i, j in group]\n        group_Y = [j for i, j in group]  # Index train_labels with integers\n        # Calculate weight based on the number of data points in the group\n        weight = len(group)\n        weights.append([weight for i in range(len(features[0]))])\n        # Select a representative data point\n        representative = np.mean(group_X, axis=0)\n        representative_label = mode(group_Y)\n        representatives.append(representative)\n        representatives_labels.append(representative_label)\n\n\n    # Replace similar data points with representatives\n    summarized_features = np.array(representatives)\n    summarized_labels = np.array(representatives_labels)\n    summarized_weights = np.array(weights)\n    \n    return summarized_features, summarized_labels, summarized_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_news = fetch_data_from_supabase(\"text_datasets\", \"true_text_dataset\", \"text\", 400)\nfalse_news = fetch_data_from_supabase(\"text_datasets\", \"false_text_dataset\", \"text\", 400)\n\n# Add labels and prepare data\nall_texts = true_news + false_news\nall_labels = [\"true\"] * len(true_news) + [\"false\"] * len(false_news)\ntrue_news = None\nfalse_news = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(all_texts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_texts, test_texts, train_labels, test_labels = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\nall_texts = None\nall_labels = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels = propagate_labels(train_texts, train_labels, test_texts)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the classifier\naccuracy = accuracy_score(test_labels, predicted_labels)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}